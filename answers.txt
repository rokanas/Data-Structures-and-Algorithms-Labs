Plagiarism detection
====================

Time spent per group member:
* Konstantinos Rokanas:
* Erik Lindstrand:
* Feride Hansson:

Task 1: Analyzing the slow program
----------------------------------

**Question**
What is the asymptotic complexity of findSimilarity?
Answer in terms of N, the total number of 5-grams in the input files.
Assume that the number of duplicate occurrences of 5-grams is a small constant - that is, there is not much plagiarised text.
Explain briefly.

findSimilarity consists of four levels of nested for-loops:
        - 2 for iterating through documents (D^2)
        - 2 for iterating through ngrams (K^2)
Therefore, the complexity of the algorithm is D^2 * K^2, or N^2 (expressed in terms of N because N = D * K).
Since the number of duplicate occurrences of 5-grams is a small constant, we do not need to account for populating the similarity BST when considering the scalability of the algorithm.


**Question**
How long did the program take on the 'small' and 'medium' directories?
Is the ratio between the times what you would expect,
given the asymptotic complexity? Explain very briefly why.

- On the small directory, the program took 2.77s.
- On the medium directory, the program took 408.21s.
We need to compare the ratio of running times between the small and medium directories to the ratio between the expected running times for 20,000 and 200,000 elements
assuming a complexity of N^2.
- The ratio between the actual times is 408.21 / 2.77 = growth factor of 147.3
- The ratio between expected times is (200,000^2) / (20,000^2) = growth factor of 100
The algorithm's running time, does, therefore, seem to increase quadratically. It runs slower than expected, but this is likely because in our predicted asymptotic complexity,
we assume that the amount of plagiarized text is constant between directories.

**Question**
How long do you predict the program would take to run on
the 'huge' directory? Show your calculations.

Using the below formula:
Time for size 4,000,000 = (time for size 200,000) * (number of operations for size 4,000,000) / (number of operations for size 200,000)

Based on our actual results:
T = 408.21s * (4,000,000^2) / (200,000^2)
T = 1,632,840,000 seconds OR 27,214,000 minutes OR 453,011.11 hours. //WRONG - somehow did calculation wrong
T = 163,284 seconds //RIGHT

Task 2: Using an index
----------------------

**Question**
Which of the three BSTs in the program usually become unbalanced?
Say very briefly:
- how you observed this,
- why you think this is the case.

TODO

**Question** (optional)
Is there a simple way to stop these trees becoming unbalanced?

TODO (optional)

Task 3: Using scapegoat trees instead of BSTs
---------------------------------------------

For the below questions, we denote by N the total number of 5-grams.
We assume there is a (small) constant number of duplicate occurrences of 5-grams.

**Question**
What is the asymptotic complexity of buildIndex?
Justify briefly.

TODO

**Question**
What is the asymptotic complexity of findSimilarity?
Justify briefly.

TODO

**Question** (optional)
Instead of the previous assumption, we now allow an arbitrary total similarity score S.
Express the asymptotic complexity of the two functions in terms of both N and S (at the same time).

TODO (optional)

Appendix: general information
=============================

**Question**
Do you know of any bugs or limitations?

TODO

**Question**
Did you collaborate with any other students on this lab?
If so, write with whom and in what way you collaborated.
Also list any resources (including the web) you have used in creating your design.

TODO

**Question**
Did you encounter any serious problems?

TODO

**Question**
What is your feedback on this assignment?

TODO
